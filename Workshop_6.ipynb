{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PART 1"
      ],
      "metadata": {
        "id": "nbGKcaZ9CJ5b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "19uLH7jOBGpt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using the sigmoid fun\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "M6240ftTCO0v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(X, y, w, b):\n",
        "    m = X.shape[0]\n",
        "    z = np.dot(X, w) + b\n",
        "    y_hat = sigmoid(z)\n",
        "\n",
        "    cost = -(1/m) * np.sum(\n",
        "        y * np.log(y_hat + 1e-9) + (1 - y) * np.log(1 - y_hat + 1e-9)\n",
        "    )\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "fwUxuvYICZpy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w, b, lr, epochs):\n",
        "    m = X.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        z = np.dot(X, w) + b\n",
        "        y_hat = sigmoid(z)\n",
        "\n",
        "        dw = (1/m) * np.dot(X.T, (y_hat - y))\n",
        "        db = (1/m) * np.sum(y_hat - y)\n",
        "\n",
        "        w -= lr * dw\n",
        "        b -= lr * db\n",
        "\n",
        "        cost = compute_cost(X, y, w, b)\n",
        "        costs.append(cost)\n",
        "\n",
        "    return w, b, costs\n"
      ],
      "metadata": {
        "id": "edFe2a5QCtom"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# binary data\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([0, 0, 0, 1, 1])\n",
        "\n",
        "w = np.zeros(X.shape[1])\n",
        "b = 0\n",
        "lr = 0.1\n",
        "epochs = 1000\n",
        "\n",
        "w, b, costs = gradient_descent(X, y, w, b, lr, epochs)\n",
        "\n",
        "print(\"Final weight:\", w)\n",
        "print(\"Final bias:\", b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2zl5GZPCwA8",
        "outputId": "0853f668-6f08-4896-9577-a53212755127"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final weight: [1.76904586]\n",
            "Final bias: -5.956350539118638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w, b):\n",
        "    probs = sigmoid(np.dot(X, w) + b)\n",
        "    return (probs >= 0.5).astype(int)\n",
        "\n",
        "predictions = predict(X, w, b)\n",
        "print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "id": "P2MlqnIsC1GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 2"
      ],
      "metadata": {
        "id": "Ki-CEJ8JC-hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n"
      ],
      "metadata": {
        "id": "xDp3d8t-DAWE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(y, num_classes):\n",
        "    m = y.shape[0]\n",
        "    one_hot_y = np.zeros((m, num_classes))\n",
        "    one_hot_y[np.arange(m), y] = 1\n",
        "    return one_hot_y\n"
      ],
      "metadata": {
        "id": "CMOg2t7VDI-n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_softmax(X, y, W, b):\n",
        "    m = X.shape[0]\n",
        "    z = np.dot(X, W) + b\n",
        "    y_hat = softmax(z)\n",
        "    cost = -(1/m) * np.sum(y * np.log(y_hat + 1e-9))\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "Cfz__lyBDLdf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_softmax(X, y, W, b, lr, epochs):\n",
        "    m = X.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        z = np.dot(X, W) + b\n",
        "        y_hat = softmax(z)\n",
        "\n",
        "        dW = (1/m) * np.dot(X.T, (y_hat - y))\n",
        "        db = (1/m) * np.sum(y_hat - y, axis=0)\n",
        "\n",
        "        W -= lr * dW\n",
        "        b -= lr * db\n",
        "\n",
        "        cost = compute_cost_softmax(X, y, W, b)\n",
        "        costs.append(cost)\n",
        "\n",
        "    return W, b, costs\n"
      ],
      "metadata": {
        "id": "Do-V37vnDP_c"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data (3 classes)\n",
        "X = np.array([[1,2],\n",
        "              [1,3],\n",
        "              [2,1],\n",
        "              [3,1],\n",
        "              [3,3]])\n",
        "\n",
        "y = np.array([0, 1, 2, 2, 1])\n",
        "num_classes = 3\n",
        "\n",
        "y_onehot = one_hot(y, num_classes)\n",
        "\n",
        "W = np.zeros((X.shape[1], num_classes))\n",
        "b = np.zeros(num_classes)\n",
        "\n",
        "lr = 0.1\n",
        "epochs = 1000\n",
        "\n",
        "W, b, costs = gradient_descent_softmax(X, y_onehot, W, b, lr, epochs)\n",
        "\n",
        "print(\"Final weights:\\n\", W)\n",
        "print(\"Final bias:\\n\", b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EkDBwaSDRm4",
        "outputId": "b39177d8-9c96-4274-b5e9-202c1241f56b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final weights:\n",
            " [[-1.65027183 -0.53882342  2.18909525]\n",
            " [ 0.44650664  2.48233898 -2.92884562]]\n",
            "Final bias:\n",
            " [ 2.3807574  -3.43799431  1.05723692]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_softmax(X, W, b):\n",
        "    z = np.dot(X, W) + b\n",
        "    y_hat = softmax(z)\n",
        "    return np.argmax(y_hat, axis=1)\n",
        "\n",
        "predictions = predict_softmax(X, W, b)\n",
        "print(\"Predicted classes:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10_fuhM9DWeP",
        "outputId": "5dda5820-d3a3-45e7-9d63-a6b2b8a3ca4a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted classes: [0 1 2 2 1]\n"
          ]
        }
      ]
    }
  ]
}